<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Assistant Client</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            height: 100vh;
            margin: 0;
            background-color: #f4f4f4;
        }
        h1 {
            color: #333;
        }
        button {
            padding: 10px 20px;
            font-size: 18px;
            margin: 10px;
            cursor: pointer;
            background-color: #4CAF50;
            color: white;
            border: none;
            border-radius: 5px;
        }
        button:disabled {
            background-color: #ccc;
        }
        .transcription {
            margin-top: 20px;
            padding: 15px;
            background-color: #fff;
            border: 1px solid #ddd;
            border-radius: 5px;
            width: 300px;
            text-align: center;
        }
    </style>
</head>
<body>

    <h1>Voice Assistant</h1>
    <button id="startButton">Start Recording</button>
    <button id="stopButton" disabled>Stop Recording</button>

    <div id="transcription" class="transcription">Your transcription will appear here...</div>

    <script>
        let socket;
        let mediaRecorder;
        let audioChunks = [];
        const startButton = document.getElementById("startButton");
        const stopButton = document.getElementById("stopButton");
        const transcriptionDiv = document.getElementById("transcription");

        startButton.addEventListener("click", async () => {
            socket = new WebSocket("ws://localhost:8888/ws");

            socket.onopen = () => {
                console.log("WebSocket connection established.");
                startRecording();
                startButton.disabled = true;
                stopButton.disabled = false;
            };

            socket.onmessage = (event) => {
                const data = JSON.parse(event.data);
                if (data.type === 'transcription') {
                    transcriptionDiv.textContent = data.text;
                } else if (data.type === 'error') {
                    transcriptionDiv.textContent = `Error: ${data.message}`;
                }
            };

            socket.onclose = () => {
                console.log("WebSocket connection closed.");
            };

            socket.onerror = (error) => {
                console.error("WebSocket error:", error);
            };
        });

        stopButton.addEventListener("click", () => {
            stopRecording();
            startButton.disabled = false;
            stopButton.disabled = true;
        });

        async function startRecording() {
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            mediaRecorder = new MediaRecorder(stream);

            mediaRecorder.ondataavailable = (event) => {
                audioChunks.push(event.data);
                // Send audio chunk immediately after it's available
                if (socket.readyState === WebSocket.OPEN) {
                    socket.send(event.data);
                }
            };

            mediaRecorder.start(250); // Record in chunks every 250ms
        }

        function stopRecording() {
            mediaRecorder.stop();

            // Inform server that recording has ended
            socket.send(JSON.stringify({ type: 'end_stream' }));

            // Close WebSocket connection after a short delay
            setTimeout(() => {
                socket.close();
            }, 500); // Adjust delay if necessary
        }
    </script>

</body>
</html>
